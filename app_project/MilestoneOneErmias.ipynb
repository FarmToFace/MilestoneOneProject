{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as Sum\n",
    "from pyspark.sql.types import ArrayType,StringType,FloatType\n",
    "from pyspark.sql.functions import col, udf, explode\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, widgets\n",
    "import pgeocode\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('MilestoneOneProject') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "master_data = spark.read.csv('../data_combined/master_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "master_data.createOrReplaceTempView(\"master_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------------+--------------------+------------------+\n",
      "|_c0|      lat|      long|raw_ingredient|               title|recipe_ingredients|\n",
      "+---+---------+----------+--------------+--------------------+------------------+\n",
      "|  0|33.789618|-110.81187|       almonds|Orange-Almond Cak...|            almond|\n",
      "|  1|33.789618|-110.81187|       almonds|         Nut Butter |            almond|\n",
      "|  2|33.789618|-110.81187|       almonds|Braised Chicken a...|            almond|\n",
      "|  3|33.789618|-110.81187|       almonds|       Jeweled Rice |            almond|\n",
      "|  4|33.789618|-110.81187|       almonds|Chocolate Almond ...|            almond|\n",
      "|  5|33.789618|-110.81187|       almonds|Roasted Winter Sq...|            almond|\n",
      "|  6|33.789618|-110.81187|       almonds|Breakfast Bowl Wi...|            almond|\n",
      "|  7|33.789618|-110.81187|       almonds|Ghostly Chocolate...|            almond|\n",
      "|  8|33.789618|-110.81187|       almonds|Toasted Almond Cr...|            almond|\n",
      "|  9|33.789618|-110.81187|       almonds|Miniature Almond ...|            almond|\n",
      "| 10|33.789618|-110.81187|       almonds|Cherry-Almond Ice...|            almond|\n",
      "| 11|33.789618|-110.81187|       almonds|Roasted Red Peppe...|            almond|\n",
      "| 12|33.789618|-110.81187|       almonds|Chicken and Necta...|            almond|\n",
      "| 13|33.789618|-110.81187|       almonds|Amaretto Olive Oi...|            almond|\n",
      "| 14|33.789618|-110.81187|       almonds|   Caramelized Nuts |            almond|\n",
      "| 15|33.789618|-110.81187|       almonds|Jam Pancakes Flam...|            almond|\n",
      "| 16|33.789618|-110.81187|       almonds|Plum-Almond Tartl...|            almond|\n",
      "| 17|33.789618|-110.81187|       almonds|Mexican Chocolate...|            almond|\n",
      "| 18|33.789618|-110.81187|       almonds|Yarnall Family's ...|            almond|\n",
      "| 19|33.789618|-110.81187|       almonds|Roasted-Almond Th...|            almond|\n",
      "+---+---------+----------+--------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the dataset\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM master_data\n",
    "    LIMIT 50\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08edccb12f864a8b8aa0e68c1502d5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Zip:', placeholder='30082')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33.8631, -84.5382)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lat_long = ''\n",
    "def get_lat_long(zip_):\n",
    "    # using a library called pgeocode\n",
    "    nomi = pgeocode.Nominatim('US')\n",
    "    geoinfo = nomi.query_postal_code(zip_)\n",
    "    lat = geoinfo.loc['latitude']\n",
    "    long = geoinfo.loc['longitude']\n",
    "    lat_long = lat, long\n",
    "    print(lat_long)\n",
    "\n",
    "def callback(wdgt):\n",
    "    lat_long = get_lat_long(wdgt.value)\n",
    "    print(lat_long)\n",
    "    \n",
    "\n",
    "text=widgets.Text(\n",
    "    value='',\n",
    "    placeholder='30082',\n",
    "    description='Zip:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(text)\n",
    "text.on_submit(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.haversine_(lat, long)>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def haversine_(lat,long):\n",
    "    user = (33.8631, -84.5382) # (lat, lon)\n",
    "    ingred = (float(lat),float(long))\n",
    "    distance = haversine(user, ingred)\n",
    "    return distance\n",
    "    \n",
    "spark.udf.register('haversine_', haversine_, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "haversine_apply_query = \"\"\"\n",
    "    SELECT lat, long, raw_ingredient, title, recipe_ingredients, haversine_(lat, long) as distance\n",
    "    FROM master_data\n",
    "    WHERE lat != 0 AND long !=0\n",
    "\"\"\"\n",
    "distance = spark.sql(haversine_apply_query)\n",
    "dist_table = distance.createOrReplaceTempView(\"dist_table\")\n",
    "# distance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+--------------------+------------------+--------+\n",
      "|      lat|      long|raw_ingredient|               title|recipe_ingredients|distance|\n",
      "+---------+----------+--------------+--------------------+------------------+--------+\n",
      "|33.789618|-110.81187|       almonds|Orange-Almond Cak...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|         Nut Butter |            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Braised Chicken a...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|       Jeweled Rice |            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Chocolate Almond ...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Roasted Winter Sq...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Breakfast Bowl Wi...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Ghostly Chocolate...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Toasted Almond Cr...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Miniature Almond ...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Cherry-Almond Ice...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Roasted Red Peppe...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Chicken and Necta...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Amaretto Olive Oi...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|   Caramelized Nuts |            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Jam Pancakes Flam...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Plum-Almond Tartl...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Mexican Chocolate...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Yarnall Family's ...|            almond|2420.308|\n",
      "|33.789618|-110.81187|       almonds|Roasted-Almond Th...|            almond|2420.308|\n",
      "+---------+----------+--------------+--------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "110 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "distance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1378.showString.\n: org.apache.spark.SparkException: Job 134 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1924)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2172)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:431)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3482)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2581)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3472)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3468)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2581)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-e94846f34bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mWHERE\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Boudin Blanc Terrine with Red Onion Confit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# groupby_min = \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \"\"\"\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1378.showString.\n: org.apache.spark.SparkException: Job 134 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1924)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2172)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:431)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3482)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2581)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3472)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3468)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2581)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "statement = \"\"\"\n",
    "    SELECT title, distance\n",
    "    FROM dist_table \n",
    "    WHERE title = \"Boudin Blanc Terrine with Red Onion Confit \"\n",
    "\"\"\"\n",
    "spark.sql(statement).show()\n",
    "\n",
    "# groupby_min = \"\"\"\n",
    "#     SELECT title, recipe_ingredients, MIN(distance) as dist\n",
    "#     FROM dist_table\n",
    "#     GROUP BY title, recipe_ingredients\n",
    "#     ORDER BY dist asc\n",
    "# \"\"\"\n",
    "# spark.sql(groupby_min).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
